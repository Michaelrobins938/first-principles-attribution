{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Causal Inference: A/B Test Validation\n",
        "\n",
        "This notebook demonstrates how to validate attribution results against ground truth from A/B tests or experiments.\n",
        "\n",
        "## The Gold Standard\n",
        "\n",
        "A/B tests provide **causal ground truth** - the actual effect of a treatment (channel) on outcomes.\n",
        "\n",
        "We can compare our attribution model's estimates against experimental results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../src')\n",
        "\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Import validation suite\n",
        "from validation import (\n",
        "    generate_synthetic_ground_truth,\n",
        "    compare_to_ground_truth,\n",
        "    run_validation_report\n",
        ")\n",
        "\n",
        "print(\"‚úì Imports successful\")\n",
        "print(\"\\nüìä This notebook covers:\")\n",
        "print(\"  1. Generate synthetic A/B test data\")\n",
        "print(\"  2. Compare attribution vs ground truth\")\n",
        "print(\"  3. Validate model accuracy\")\n",
        "print(\"  4. Interpret results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Generate Synthetic Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic data with known ground truth\n",
        "journeys, true_effects = generate_synthetic_ground_truth(\n",
        "    n_journeys=10000,\n",
        "    noise_level=0.1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"‚úì Generated synthetic experiment data\")\n",
        "print(f\"\\nüìä Dataset:\")\n",
        "print(f\"   Journeys: {len(journeys):,}\")\n",
        "print(f\"   Conversions: {sum(1 for j in journeys if j['converted']):,}\")\n",
        "print(f\"   Conversion rate: {sum(1 for j in journeys if j['converted'])/len(journeys):.1%}\")\n",
        "\n",
        "print(f\"\\nüéØ Ground Truth Causal Effects:\")\n",
        "for ch, effect in sorted(true_effects.items(), key=lambda x: -x[1]):\n",
        "    print(f\"   {ch}: {effect:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Simulate Attribution Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate attribution model output (with some error to be realistic)\n",
        "# This represents what the Markov-Shapley model would produce\n",
        "\n",
        "model_output = {\n",
        "    'Search': 0.38,   # True: 0.35 (slight overestimate)\n",
        "    'Email': 0.22,    # True: 0.25 (slight underestimate)\n",
        "    'Direct': 0.21,   # True: 0.20 (slight overestimate)\n",
        "    'Social': 0.11,   # True: 0.12 (close)\n",
        "    'Display': 0.08   # True: 0.08 (exact)\n",
        "}\n",
        "\n",
        "model_cis = {\n",
        "    'Search': (0.32, 0.44),\n",
        "    'Email': (0.18, 0.28),\n",
        "    'Direct': (0.16, 0.26),\n",
        "    'Social': (0.07, 0.15),\n",
        "    'Display': (0.04, 0.12)\n",
        "}\n",
        "\n",
        "print(\"‚úì Attribution model output ready\")\n",
        "print(\"\\nüìà Model Estimates vs Ground Truth:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Channel':<10} {'Model':>8} {'True':>8} {'Error':>10}\")\n",
        "print(\"-\" * 50)\n",
        "for ch in sorted(true_effects.keys()):\n",
        "    model = model_output[ch]\n",
        "    truth = true_effects[ch]\n",
        "    error = model - truth\n",
        "    print(f\"{ch:<10} {model:>7.1%} {truth:>7.1%} {error:>+8.1%}\")\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Validate Against Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run validation against ground truth\n",
        "result = compare_to_ground_truth(model_output, true_effects, model_cis)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"VALIDATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìä Model Performance Metrics:\")\n",
        "print(f\"   Rank Correlation (Spearman's œÅ): {result.rank_correlation:.3f}\")\n",
        "print(f\"   Mean Absolute Percentage Error: {result.magnitude_error:.1%}\")\n",
        "print(f\"   Top Channel Match: {'‚úì YES' if result.top_channel_match else '‚úó NO'}\")\n",
        "print(f\"   Top-3 Overlap: {result.top_k_overlap:.0%}\")\n",
        "print(f\"   Confidence Calibration: {result.confidence_calibration:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INTERPRETATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if result.rank_correlation > 0.8:\n",
        "    print(\"\\n‚úÖ Strong ranking agreement with ground truth\")\n",
        "elif result.rank_correlation > 0.5:\n",
        "    print(\"\\n‚ö†Ô∏è Moderate ranking agreement\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Weak ranking - model may need tuning\")\n",
        "\n",
        "if result.magnitude_error < 0.15:\n",
        "    print(\"‚úÖ Low magnitude error - accurate attribution values\")\n",
        "elif result.magnitude_error < 0.30:\n",
        "    print(\"‚ö†Ô∏è Moderate magnitude error\")\n",
        "else:\n",
        "    print(\"‚ùå High magnitude error - significant bias detected\")\n",
        "\n",
        "if result.top_channel_match:\n",
        "    print(\"‚úÖ Correctly identified the top-performing channel\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to identify the top-performing channel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Full Validation Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive validation report\n",
        "report = run_validation_report(model_output, journeys, true_effects, model_cis)\n",
        "\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Correlation vs Causation**: Attribution models capture correlation patterns.\n",
        "   A/B tests reveal actual causal effects.\n",
        "\n",
        "2. **Validation is Essential**: Always validate against ground truth when possible.\n",
        "\n",
        "3. **Limitations**: Observational attribution cannot replace experiments.\n",
        "   Use attribution for exploration, experiments for confirmation.\n",
        "\n",
        "## When to Use Each Approach\n",
        "\n",
        "| Scenario | Best Approach |\n",
        "|----------|---------------|\n",
        "| A/B test available | Use experimental results |\n",
        "| No experiment | Use attribution model |\n",
        "| Both available | Compare for validation |\n",
        "| Strategic planning | Attribution + sensitivity analysis |\n",
        "| Tactical optimization | A/B tests |\n",
        "\n",
        "---\n",
        "\n",
        "**Next:** See `../llm_analysis/03_ai_interpretation.ipynb` for AI-powered analysis of these results."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}